{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"Single layer BLSTM with Word Embedding.ipynb","provenance":[{"file_id":"1B650crgu866Oo9RPdYJmlkStbk2gRAEQ","timestamp":1589338853871},{"file_id":"1Rnuit19de-gvcs0BfcxBi5E1iC2a1WMg","timestamp":1589332980276},{"file_id":"1YPuWIYxvIp_QV4_uKY00icYlhk0jICiS","timestamp":1588467575694},{"file_id":"14Ob2PH915izdcAo8EH_VbPacmOLDr4oz","timestamp":1588452328782},{"file_id":"1WbL7Sfkp1tGHJtdgX_fJGYL-c81f7-c-","timestamp":1588452088482}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gk5Ms6MeRwAU","colab_type":"text"},"source":["## Importing the Data"]},{"cell_type":"code","metadata":{"id":"ZC3cOlLibMNo","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7xpVpJBFSNZk","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TbbxbJGw6-kL","colab_type":"code","colab":{}},"source":["# !gunzip '/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/GoogleNews-vectors-negative300.bin.gz'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BwgSxNjPRwAV","colab_type":"code","colab":{}},"source":["# Constants\n","DATASET_DIR = '/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/data/'\n","\n","import os\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n","minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n","maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]\n","new_y = []\n","for i in X.index:\n","  val = X.iloc[i]\n","  min_score = minimum_scores[int(val['essay_set'])]\n","  max_score = maximum_scores[int(val['essay_set'])]\n","  new_y.append((val['domain1_score'] - min_score)/(max_score - min_score))\n","\n","new_y = np.array(new_y)\n","# new_y = np.around(new_y*10)\n","normalized_y = pd.Series(new_y)#, dtype = 'int32')\n","\n","y = X['domain1_score']\n","X = X.dropna(axis=1)\n","X = X.drop(columns=['rater1_domain1', 'rater2_domain1', 'domain1_score', 'essay_id'])\n","X_train, X_test, y_train, y_test = train_test_split(X, normalized_y, test_size=0.2, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gzvQs1NdDLO6","colab_type":"code","colab":{}},"source":["y_train"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QCGsuG8nRwAn","colab_type":"text"},"source":["# Preprocessing the Data"]},{"cell_type":"markdown","metadata":{"id":"WduYRrZ49MR2","colab_type":"text"},"source":["Below are our implementations of various functions to tokenise, pad and generate the encoded train for word embeddings."]},{"cell_type":"code","metadata":{"id":"qsIaZZ9uRwAp","colab_type":"code","colab":{}},"source":["import numpy as np\n","import nltk\n","from nltk.corpus import stopwords\n","import re\n","import keras\n","from nltk.tokenize import RegexpTokenizer\n","from keras.preprocessing.sequence import  pad_sequences\n","from nltk import FreqDist\n","# import contractions\n","\n","# nltk.download('punkt')\n","# nltk.download('stopwords')\n","# tokenizer = RegexpTokenizer(r'\\w+')\n","# num_regex = re.compile('^[+-]?[0-9]+\\.?[0-9]*$')\n","# vocab_size = 4000\n","# def is_number(token):\n","# \treturn bool(num_regex.match(token))\n"," \n","# def tokenise(essay, remove_stopwords=False):\n","#   tokens = tokenizer.tokenize(contractions.fix(essay.lower()))\n","#   if remove_stopwords:\n","#     tokens =  [word for word in tokens if not word in stopwords.words()]\n","#   return tokens\n","\n","# def create_vocab(essays, vocab_size=vocab_size):\n","#   total_tokens = []\n","#   for essay in essays:\n","#     tokens = tokenise(essay)\n","#     total_tokens += tokens\n","#   # print(len(total_tokens))\n","  \n","#   vocab = {'<pad>':0, '<unk>':1, '<num>':2}\n","#   vcb_len = len(vocab)\n","#   index = vcb_len\n","#   for word, _ in FreqDist(total_tokens).most_common():\n","#     # if word in stopwords.words():\n","#     #   continue\n","#     vocab[word] = index\n","#     index += 1\n","#     if index == vocab_size:\n","#       break\n","#   del total_tokens\n","#   return vocab\n","\n","# def convert_data(essays, vocab, max_length=0):\n","#   encoded_essays = []\n","#   maxlen = 0\n","#   for essay in essays:\n","#     encoded_essay = []\n","#     tokens = tokenise(essay)\n","#     maxlen = max(maxlen, len(tokens))\n","#     for word in tokens:\n","#       if is_number(word):\n","#         encoded_essay.append(vocab['<num>'])\n","#       elif word in vocab:\n","#         encoded_essay.append(vocab[word])\n","#       else:\n","#         encoded_essay.append(vocab['<unk>'])\n","#     encoded_essays.append(encoded_essay)\n","#   if max_length == 0:\n","#     max_length = maxlen  \n","#   return pad_sequences(encoded_essays, maxlen=max_length, padding='post'), max_length"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qfxBurRHflCf","colab_type":"code","colab":{}},"source":["# vocab = create_vocab(X_train['essay'].values)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSk4n05h7I53","colab_type":"code","colab":{}},"source":["# emb_dim = 50\n","# embeddings = {}\n","# emb_file = open('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/En_vectors.txt')\n","# lines = emb_file.readlines()\n","# for line in lines:\n","#   tokens = line.split()\n","#   word = tokens[0]\n","#   vec = tokens[1].split(',')\n","#   embeddings[word] = vec\n","\n","# def get_emb_matrix_given_vocab(vocab, embeddings):\n","#   counter = 0\n","#   emb_matrix = np.zeros((len(vocab), emb_dim))\n","#   for word, index in vocab.items():\n","#     try:\n","#       emb_matrix[index] = embeddings[word]\n","#       counter += 1\n","#     except KeyError:\n","#       pass\n","#   print(counter, len(vocab), 100*counter/len(vocab))\n","#   return emb_matrix\n","# emb_matrix = get_emb_matrix_given_vocab(vocab, embeddings)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UtcY59B35k6M","colab_type":"code","colab":{}},"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import  pad_sequences\n","train_essays = X_train['essay']\n","t = Tokenizer(num_words=4000)\n","t.fit_on_texts(list(train_essays))\n","\n","encoded_train = t.texts_to_sequences(list(train_essays))\n","max_length = 550\n","emb_dim = 50\n","padded_train = pad_sequences(encoded_train, maxlen=max_length, padding='post')\n","vocab_size = 4000\n","\n","test_essays = X_test['essay']\n","encoded_test = t.texts_to_sequences(list(test_essays))\n","padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b6UmUXu_gAw5","colab_type":"code","colab":{}},"source":["def mean_quadratic_weighted_kappa(kappas, weights=None):\n","    \"\"\"\n","    Calculates the mean of the quadratic\n","    weighted kappas after applying Fisher's r-to-z transform, which is\n","    approximately a variance-stabilizing transformation.  This\n","    transformation is undefined if one of the kappas is 1.0, so all kappa\n","    values are capped in the range (-0.999, 0.999).  The reverse\n","    transformation is then applied before returning the result.\n","    \n","    mean_quadratic_weighted_kappa(kappas), where kappas is a vector of\n","    kappa values\n","    mean_quadratic_weighted_kappa(kappas, weights), where weights is a vector\n","    of weights that is the same size as kappas.  Weights are applied in the\n","    z-space\n","    \"\"\"\n","    kappas = np.array(kappas, dtype=float)\n","    if weights is None:\n","        weights = np.ones(np.shape(kappas))\n","    else:\n","        weights = weights / np.mean(weights)\n","\n","    # ensure that kappas are in the range [-.999, .999]\n","    kappas = np.array([min(x, .999) for x in kappas])\n","    kappas = np.array([max(x, -.999) for x in kappas])\n","    \n","    z = 0.5 * np.log( (1+kappas)/(1-kappas) ) * weights\n","    z = np.mean(z)\n","    kappa = (np.exp(2*z)-1) / (np.exp(2*z)+1)\n","    return kappa"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-LLaY340nJK","colab_type":"code","colab":{}},"source":["def evaluatePreds(y_test, y_pred, X_test):\n","  print('*'*100)\n","  print(\"Calculating Kappa Scores....\")\n","  new_y_test = np.copy(y_test.values)\n","  new_y_pred = np.squeeze(np.copy(y_pred))\n","  for i, e_set in enumerate(X_test['essay_set'].values):\n","    min_score = minimum_scores[int(e_set)]\n","    max_score = maximum_scores[int(e_set)]\n","    new_y_test[i] = new_y_test[i]*(max_score - min_score) + min_score\n","    new_y_pred[i] = new_y_pred[i]*(max_score - min_score) + min_score\n","  new_y_pred = np.around(new_y_pred)\n","  new_y_test = np.around(new_y_test)\n","  finalResult = []\n","  for i in range(1,9):\n","    finalResult.append(cohen_kappa_score(new_y_test[X_test['essay_set'] == i],new_y_pred[X_test['essay_set'] == i],weights='quadratic'))\n","  print(finalResult)\n","  print(\"Average QWK (The one that matters):\", mean_quadratic_weighted_kappa(finalResult))\n","  print('*'*100)\n","  return finalResult"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hVZIm0k_u5H3","colab_type":"code","colab":{}},"source":["from sklearn.metrics import cohen_kappa_score\n","from datetime import datetime\n","import os\n","import csv\n","\n","class CheckKappa(keras.callbacks.Callback):\n","    def __init__(self, interval=10, custom_filename=None, is_continue=False, continue_from=None):\n","        super(keras.callbacks.Callback, self).__init__()\n","        self.interval = interval\n","        self.custom_filename = custom_filename\n","        self.is_continue = is_continue\n","        self.continue_from = continue_from\n","\n","    def on_train_begin(self, logs={}):\n","        self.scores = []\n","        self.losses = []\n","        self.filename = \"\"\n","        dir_path = \"/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/train_log/\"\n","        if not os.path.exists(dir_path):\n","          os.makedirs(dir_path)\n","        model_name = \"seq\" + str(max_length) + \"_emb\" + str(emb_dim)\n","        if self.custom_filename is None:\n","          self.filename = os.path.join(dir_path, \"train_\" + datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p_\")) + model_name + '.csv'\n","        else:\n","          self.filename = os.path.join(dir_path, self.custom_filename)\n","\n","        self.fieldnames = ['Epoch', 'Train_Loss', 'mae', 'Score']\n","\n","        if not self.is_continue:\n","          with open(self.filename, 'a') as csvfile:\n","            writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)\n","            writer.writeheader()\n","        else:\n","          log = pd.read_csv(self.filename)\n","          if self.continue_from is not None:\n","            log[log['Epoch'] <= self.continue_from].to_csv(self.filename, index=False)\n","            print(\"Continuing Training after Epoch {0}\".format(self.continue_from))\n","\n","    def on_train_end(self, logs={}):\n","        return\n"," \n","    def on_epoch_begin(self, epoch, logs={}):\n","        return\n"," \n","    def on_epoch_end(self, epoch, logs={}):\n","        self.losses.append(logs.get('loss'))\n","        self.scores.append(-1)\n","        cur_epoch = epoch\n","        if self.continue_from is not None:\n","          cur_epoch += self.continue_from\n","        row = {\"Epoch\": cur_epoch + 1, \"Train_Loss\": logs.get('loss'), \"mae\": logs.get('mae'), \"Score\": np.nan}\n","\n","        if (cur_epoch + 1) % self.interval == 0:\n","          y_pred = self.model.predict(padded_test)\n","          finalResult = evaluatePreds(y_test, y_pred, X_test)\n","          row[\"Score\"] = mean_quadratic_weighted_kappa(finalResult)\n","          self.scores[-1] = mean_quadratic_weighted_kappa(finalResult)\n","\n","        with open(self.filename , 'a') as csvfile:\n","          writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)\n","          writer.writerow(row)\n","\n","        return"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xINb9EWaRwA0","colab_type":"text"},"source":["## Defining the model "]},{"cell_type":"code","metadata":{"id":"a1fP8Y6KX_4U","colab_type":"code","colab":{}},"source":["pretrained_embeddings = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MgEARrqCRwA2","colab_type":"code","colab":{}},"source":["from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten, Bidirectional\n","from keras.models import Sequential, load_model, model_from_config\n","import keras.backend as K\n","from keras.optimizers import RMSprop\n","from keras.initializers import Constant\n","def get_model():\n","    \"\"\"Define the model.\"\"\"\n","    model = Sequential()\n","    if pretrained_embeddings:\n","      model.add(Embedding(vocab_size, emb_dim, mask_zero=True, embeddings_initializer=Constant(emb_matrix)))\n","    else:\n","      model.add(Embedding(vocab_size, emb_dim, mask_zero=True))\n","    model.add(Bidirectional(LSTM(300, dropout=0.5, recurrent_dropout=0.1, return_sequences=False)))\n","    # model.add(Bidirectional(LSTM(64, recurrent_dropout=0.1, return_sequences=False)))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(loss='mean_squared_error', optimizer=RMSprop(lr=1e-3), metrics=['mae'])\n","    model.summary()\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"baG0KybaRwA6","colab_type":"text"},"source":["## Training Phase"]},{"cell_type":"code","metadata":{"id":"6Zz3aQmnv3Z6","colab_type":"code","colab":{}},"source":["# train_essays, max_length = convert_data(X_train['essay'].values, vocab, max_length=250)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CFVIAgnbOM3r","colab_type":"code","colab":{}},"source":["# Check Existing Logs\n","# !ls \"/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/train_log/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKOrZdI1T1w8","colab_type":"code","colab":{}},"source":["# Clear Logs\n","# !rm \"/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/train_log/\"*\n","# !rm \"/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/saved_models/lstm_model_epoch\"*"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yRtrSiUaSAAr","colab_type":"code","outputId":"29f1db25-42cb-4244-87bc-149e40acbf1d","executionInfo":{"status":"ok","timestamp":1589313539615,"user_tz":240,"elapsed":3408,"user":{"displayName":"Abhinav Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjD2WsABYqG0avc38mtGCaFZhmT_fZhK2GT-cwZ=s64","userId":"15452217471361756471"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["# check saved models\n","# !ls \"/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/saved_models/\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["emb200_seq500.h5\tlstm_model_epoch-25.h5\tlstm_model_epoch-65.h5\n","emb300_seq500.h5\tlstm_model_epoch-30.h5\tlstm_model_epoch-70.h5\n","emb50_seq500.h5\t\tlstm_model_epoch-35.h5\tlstm_model_epoch-75.h5\n","final_lstm.h5\t\tlstm_model_epoch-40.h5\tlstm_model_epoch-80.h5\n","lstm_model_epoch-05.h5\tlstm_model_epoch-45.h5\tlstm_model_epoch-85.h5\n","lstm_model_epoch-10.h5\tlstm_model_epoch-50.h5\tlstm_model_epoch-90.h5\n","lstm_model_epoch-15.h5\tlstm_model_epoch-55.h5\n","lstm_model_epoch-20.h5\tlstm_model_epoch-60.h5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MBLfCP7hSKvo","colab_type":"code","colab":{}},"source":["# copy saved models\n","# !cp \"/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/saved_models/lstm_model_epoch-100.hf5\" \"/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/saved_models/emb200_seq500.h5\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yF0o1FkBzOxT","colab_type":"code","colab":{}},"source":["from keras.callbacks import ModelCheckpoint\n","\n","filepath=\"/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/saved_models/lstm_model_epoch-{epoch:02d}.h5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='train_loss', verbose=1, period=5)\n","checkpoint_log = CheckKappa(interval=5)       # If resuming training, custom_filename= <Existing Log name> and is_continue=True\n","callbacks_list = [checkpoint_log, checkpoint]\n","lstm_model = get_model()\n","# lstm_model = load_model('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/saved_models/lstm_model_epoch-90.h5')\n","history = lstm_model.fit(padded_train, y_train, batch_size=64, epochs=100, callbacks=callbacks_list)\n","\n","# lstm_model.save('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/saved_models/final_lstm.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UQlfmC0pPq5G","colab_type":"code","colab":{}},"source":["# Plot Train Loss, mae and Score\n","import matplotlib.pyplot as plt\n","log = pd.read_csv(\"/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/train_logs/\" + \"train_07-05-2020_08-45-54_AM_seq500_emb50.csv\")\n","plt.plot(log['Epoch'], log['Train_Loss'], c='b', label='loss')\n","plt.plot(log['Epoch'], log['mae'], c='r', label='mae')\n","plt.legend()\n","plt.savefig('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/embedding/outputs/300d_runs/swap.png')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJQs3RFppUBS","colab_type":"code","colab":{}},"source":["plt.figure()\n","plt.plot(log.dropna()['Epoch'], log.dropna()['Score'], label='QWK score')\n","plt.legend()\n","plt.savefig('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/embedding/outputs/300d_runs/swap_result.png')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8zYRuTSXWxVq","colab_type":"code","colab":{}},"source":["log[log['Epoch']==70]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V0JWZLv8RwBD","colab_type":"code","colab":{}},"source":["lstm_model_final = load_model('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/saved_models/emb50_seq500.h5')\n","\n","# Generate training and testing data word vectors.\n","\n","split_y_pred = lstm_model_final.predict(padded_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MAoqY0ltxbYU","colab_type":"code","colab":{}},"source":["kappas = evaluatePreds(y_test, split_y_pred, X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKUtGXGNaxtA","colab_type":"code","colab":{}},"source":["# !cp '/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/saved_models/lstm_model_epoch-80.h5' '/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/saved_models/emb300_seq500.h5'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Q3FSuSOvtpa","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","loss_arr = history.history['loss']\n","plt.plot(loss_arr, c='b', label='Loss')\n","plt.legend()\n","plt.savefig('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/outputs/validationLoss_vs_epoch_1_swap.png')\n","\n","plt.figure()\n","mae_arr = history.history['mae']\n","plt.plot(mae_arr, c='r', label='mae')\n","plt.legend()\n","plt.savefig('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/outputs/mae_vs_epoch_1_swap.png')\n","\n","plt.figure()\n","plt.plot(loss_arr, c='b', label='Loss')\n","plt.plot(mae_arr, c='r', label='mae')\n","plt.legend()\n","plt.savefig('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/outputs/mae_val_loss_vs_epoch_1_swap.png')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OpHZ8vfLnYmE","colab_type":"code","colab":{}},"source":["import pickle\n","\n","def save_obj(obj, name ):\n","    with open('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/history/'+ name + '.pkl', 'wb') as f:\n","        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n","\n","def load_obj(name ):\n","    with open('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/history/' + name + '.pkl', 'rb') as f:\n","        return pickle.load(f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rV3nvZv_nZ9D","colab_type":"code","colab":{}},"source":["save_obj(history.history, \"history_dict_1-100_swap\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wuJSQdFibeCv","colab_type":"code","outputId":"93c5cfb7-10ee-4f9f-caaa-c745ee8eb0ff","executionInfo":{"status":"ok","timestamp":1589312885793,"user_tz":240,"elapsed":890,"user":{"displayName":"Abhinav Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjD2WsABYqG0avc38mtGCaFZhmT_fZhK2GT-cwZ=s64","userId":"15452217471361756471"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["mean_quadratic_weighted_kappa(kappas)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6742082609877676"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"1LE_YWpOW9SE","colab_type":"text"},"source":["Original Code: https://github.com/nusnlp/nea/blob/master/nea/models.py"]}]}
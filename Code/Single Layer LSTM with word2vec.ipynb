{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"Single Layer LSTM with word2vec.ipynb","provenance":[{"file_id":"1YPuWIYxvIp_QV4_uKY00icYlhk0jICiS","timestamp":1589337343225},{"file_id":"14Ob2PH915izdcAo8EH_VbPacmOLDr4oz","timestamp":1588452328782},{"file_id":"1WbL7Sfkp1tGHJtdgX_fJGYL-c81f7-c-","timestamp":1588452088482}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"gk5Ms6MeRwAU","colab_type":"text"},"source":["## Importing the Data"]},{"cell_type":"code","metadata":{"id":"7xpVpJBFSNZk","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TbbxbJGw6-kL","colab_type":"code","colab":{}},"source":["# !gunzip '/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/GoogleNews-vectors-negative300.bin.gz'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BwgSxNjPRwAV","colab_type":"code","colab":{}},"source":["# Constants\n","DATASET_DIR = '/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/data/'\n","SAVE_DIR = './'\n","\n","import os\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","  \n","X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n","y = X['domain1_score']\n","X = X.dropna(axis=1)\n","X = X.drop(columns=['rater1_domain1', 'rater2_domain1', 'domain1_score', 'essay_id'])\n","X, split_X_test, y, split_y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_WBUTaU7K-s","colab_type":"code","colab":{}},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QCGsuG8nRwAn","colab_type":"text"},"source":["# **Helper Functions to Process Data**\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"qsIaZZ9uRwAp","colab_type":"code","colab":{}},"source":["import numpy as np\n","import nltk\n","import re\n","from nltk.corpus import stopwords\n","from gensim.models import Word2Vec\n","\n","def getWords(essay_v, remove_stopwords):\n","    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n","    words = essay_v.lower().split()\n","    if remove_stopwords:\n","        stops = set(stopwords.words(\"english\"))\n","        words = [w for w in words if not w in stops]\n","    return (words)\n","\n","def getSentences(essay_v, remove_stopwords):\n","    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n","    raw_sentences = tokenizer.tokenize(essay_v.strip())\n","    sentences = []\n","    for raw_sentence in raw_sentences:\n","        if len(raw_sentence) > 0:\n","            sentences.append(getWords(raw_sentence, remove_stopwords))\n","    return sentences\n","\n","def makeFeatureVec(words, model, num_features):\n","    featureVec = np.zeros((num_features,),dtype=\"float32\")\n","    num_words = 0.\n","    index2word_set = set(model.wv.index2word)\n","    for word in words:\n","        if word in index2word_set:\n","            num_words += 1\n","            featureVec = np.add(featureVec,model[word])        \n","    featureVec = np.divide(featureVec,num_words)\n","    return featureVec\n","\n","def getAvgFeatureVecs(essays, model, num_features):\n","    counter = 0\n","    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n","    for essay in essays:\n","        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n","        counter = counter + 1\n","    return essayFeatureVecs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xINb9EWaRwA0","colab_type":"text"},"source":["## LSTM model "]},{"cell_type":"markdown","metadata":{"id":"Jg2l1pQwRwA1","colab_type":"text"},"source":["We utilise a 3-layer LSTM model with relu as the activation function. This is because the scores are not normalised and do not range between 0 and 1"]},{"cell_type":"code","metadata":{"id":"MgEARrqCRwA2","colab_type":"code","colab":{}},"source":["from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n","from keras.models import Sequential, load_model, model_from_config\n","import keras.backend as K\n","\n","def get_model():\n","    \"\"\"Define the model.\"\"\"\n","    model = Sequential()\n","    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n","    model.add(LSTM(200, dropout=0.4, recurrent_dropout=0.4, return_sequences=True))\n","    model.add(LSTM(64, recurrent_dropout=0.4))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='relu'))\n","    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n","    model.summary()\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"baG0KybaRwA6","colab_type":"text"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"anhhSl0SRwA7","colab_type":"text"},"source":["Now we train the model on the dataset.\n","\n","We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n","We will then calculate Average Kappa for all the folds."]},{"cell_type":"markdown","metadata":{"id":"IVMalUKz7tXg","colab_type":"text"},"source":["Importing pretrained word2vec model"]},{"cell_type":"code","metadata":{"id":"trqg2IX7Devy","colab_type":"code","colab":{}},"source":["# import gensimts/NLP/automated_essay_grading/GoogleNews-vectors-negative300.bin', binary=True)  \n","# word2Vec_model = gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/Projec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6PVlqSYdVHY6","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SU7-ZNPhRwA7","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import KFold\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import cohen_kappa_score\n","import matplotlib.pyplot as plt\n","\n","nltk.download('punkt')\n","cv = KFold(n_splits = 4, shuffle = True)\n","results = []\n","y_pred_list = []\n","\n","count = 1\n","\n","max_result = 0.0\n","collected_mae = []\n","collected_epoch_results = []\n","collected_loss_history = []\n","\n","for traincv, testcv in cv.split(X):\n","    print(\"\\n--------Fold {}--------\\n\".format(count))\n","    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n","    \n","    train_essays = X_train['essay']\n","    test_essays = X_test['essay']\n","    \n","    sentences = []\n","    \n","    for essay in train_essays:\n","            sentences += getSentences(essay, remove_stopwords = True)\n","            \n","    num_features = 300 \n","    min_word_count = 40\n","    num_workers = 4\n","    context = 10\n","    downsampling = 1e-3\n","\n","    print(\"Training Word2Vec Model...\")\n","    word2Vec_model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n","\n","    word2Vec_model.init_sims(replace=True)\n","    word2Vec_model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n","\n","    clean_train_essays = []\n","    \n","    for essay_v in train_essays:\n","        clean_train_essays.append(getWords(essay_v, remove_stopwords=True))\n","    trainDataVecs = getAvgFeatureVecs(clean_train_essays, word2Vec_model, num_features)\n","    \n","    clean_test_essays = []\n","    for essay_v in test_essays:\n","        clean_test_essays.append(getWords( essay_v, remove_stopwords=True ))\n","    testDataVecs = getAvgFeatureVecs( clean_test_essays, word2Vec_model, num_features )\n","    \n","    trainDataVecs = np.array(trainDataVecs)\n","    testDataVecs = np.array(testDataVecs)\n","    print(trainDataVecs.shape)\n","\n","    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n","    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n","    \n","    lstm_model = get_model()\n","    \n","    epochs = 50\n","    epoch_results = []\n","    loss_history = []\n","    mae = []\n","    for i in range(0,epochs):\n","      history = lstm_model.fit(trainDataVecs, y_train, batch_size=32)\n","      y_pred1 = lstm_model.predict(testDataVecs)\n","      y_pred1 = np.around(y_pred1)\n","      result = cohen_kappa_score(y_test.values,y_pred1,weights='quadratic')\n","      loss_history.append(history.history['loss'])\n","      epoch_results.append(result)\n","      mean_error = history.history['mae']\n","      mae.append(mean_error)\n","\n","    collected_epoch_results.append(epoch_results)\n","    collected_loss_history.append(loss_history)\n","    collected_mae.append(mae)\n","    \n","    loss_train = history.history['loss']\n","    \n","    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n","    y_pred = lstm_model.predict(testDataVecs)\n","    \n","    y_pred = np.around(y_pred)\n","    \n","    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n","\n","    if max_result < result :\n","      lstm_model.save('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/model_weights/final_lstm.h5')\n","      max_result = result\n","\n","    print(\"Kappa Score: {}\".format(result))\n","    results.append(result)\n","\n","    count += 1\n","    # break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3P844FIDRwA_","colab_type":"code","colab":{}},"source":["print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ktQb3-kS0X-o","colab_type":"code","colab":{}},"source":["# Plotting graphs for loss of every epoch for each fold (4 folds)\n","import matplotlib.pyplot as plt\n","\n","plt.plot(collected_loss_history[0], c='r', label='loss_fold1')\n","plt.plot(collected_loss_history[1], c='b', label='loss_fold2')\n","plt.plot(collected_loss_history[2], c='y', label='loss_fold3')\n","plt.plot(collected_loss_history[3], c='g', label='loss_fold4')\n","plt.legend()\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"training_loss\")\n","plt.savefig('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/Word2Vec/outputs/300d/training_loss_vs_epoch.png')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MqjPtFBS2AoQ","colab_type":"code","colab":{}},"source":["# Plotting graphs for loss of every epoch for each fold (4 folds)\n","\n","plt.plot(collected_mae[0], c='r', label='mae_fold1')\n","plt.plot(collected_mae[1], c='b', label='mae_fold2')\n","plt.plot(collected_mae[2], c='y', label='mae_fold3')\n","plt.plot(collected_mae[3], c='g', label='mae_fold4')\n","plt.legend()\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"mean_squared_error\")\n","plt.savefig('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/Word2Vec/outputs/300d/mean_squared_error_vs_epoch.png')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1paEEAIw6UBj","colab_type":"code","colab":{}},"source":["# Plotting graphs for kappa results of every epoch for each fold (4 folds)\n","import matplotlib.pyplot as plt\n","\n","plt.plot(collected_epoch_results[0], c='r', label='kappascore_fold1')\n","plt.plot(collected_epoch_results[1], c='b', label='kappascore_fold2')\n","plt.plot(collected_epoch_results[2], c='y', label='kappascore_fold3')\n","plt.plot(collected_epoch_results[3], c='g', label='kappascore_fold4')\n","plt.legend()\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"kappa_score\")\n","plt.savefig('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/Word2Vec/outputs/300d/kappa_scores_vs_epoch.png')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4gFRPhtrt9Ir","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"fiIjmTrM26iG","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","loss_arr = history.history['loss']\n","plt.plot(loss_arr, c='b', label='Loss')\n","plt.legend()\n","plt.savefig('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/outputs/validationLoss_vs_epoch.png')\n","\n","plt.figure()\n","mae_arr = history.history['mae']\n","plt.plot(mae_arr, c='r', label='mae')\n","plt.legend()\n","plt.savefig('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/outputs/mae_vs_epoch.png')\n","\n","plt.figure()\n","plt.plot(loss_arr, c='b', label='Loss')\n","plt.plot(mae_arr, c='r', label='mae')\n","plt.legend()\n","plt.savefig('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/outputs/mae_val_loss_vs_epoch.png')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V0JWZLv8RwBD","colab_type":"code","colab":{}},"source":["\n","lstm_model_final = get_model()\n","lstm_model_final.load_weights('/content/gdrive/My Drive/Projects/NLP/automated_essay_grading/model_weights/final_lstm.h5')\n","\n","# trainSet = pd.read_csv(os.path.join(DATASET_DIR, 'test_set.tsv'), sep='\\t', encoding='ISO-8859-1')\n","\n","# Generate training and testing data word vectors.\n","test_essays = split_X_test['essay']\n","clean_test_essays = []\n","for essay_v in test_essays:\n","    clean_test_essays.append(getWords( essay_v, remove_stopwords=True ))\n","testDataVecs = getAvgFeatureVecs( clean_test_essays, word2Vec_model, num_features )\n","\n","testDataVecs = np.array(testDataVecs)\n","# Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n","testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n","\n","split_y_pred = lstm_model.predict(testDataVecs)\n","\n","# Round y_pred to the nearest integer.\n","split_y_pred = np.around(split_y_pred)\n","\n","finalResult = cohen_kappa_score(split_y_test.values,split_y_pred,weights='quadratic')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4RWBUDPHxFD_","colab_type":"code","colab":{}},"source":["print(finalResult)"],"execution_count":0,"outputs":[]}]}